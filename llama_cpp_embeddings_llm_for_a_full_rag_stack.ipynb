{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvlIDpCHSJPLQdieSmfhvP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edcalderin/LLM_Tech/blob/master/llama_cpp_embeddings_llm_for_a_full_rag_stack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama.cpp one man band! Embeddings + LLM for a full RAG stack"
      ],
      "metadata": {
        "id": "AAS5QsUNJDdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://generativeai.pub/llama-cpp-one-man-band-embeddings-llm-for-a-full-rag-stack-435be8e05b2b"
      ],
      "metadata": {
        "id": "M500hvk-JG_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The application uses the fastest and accurate Small Language Model (`Qwen2`) and `all-MiniLM-L6-v2` embeddings from the `sentence-transformers` stack"
      ],
      "metadata": {
        "id": "AkvhCeBPJIV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken PyMuPDF langchain-community llama-cpp-python faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCuyA1sGJQej",
        "outputId": "3fed229c-6f62-40cb-f4ee-474616c25e65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "from langchain_text_splitters import TokenTextSplitter, CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import LlamaCppEmbeddings\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "GJ-F7-M9NsCo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://huggingface.co/unsloth/Qwen3-4B-128K-GGUF/resolve/main/Qwen3-4B-128K-Q8_0.gguf\n",
        "!wget -nc https://huggingface.co/leliuga/all-MiniLM-L6-v2-GGUF/resolve/main/all-MiniLM-L6-v2.F16.gguf\n",
        "!wget -nc https://raw.githubusercontent.com/edcalderin/salesorder-sqlchatbot/refs/heads/master/README.md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnBaHfHVdiPc",
        "outputId": "3f7aac76-516d-4936-a96b-e4b563fda8f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File â€˜Qwen3-4B-128K-Q8_0.ggufâ€™ already there; not retrieving.\n",
            "\n",
            "--2025-07-31 19:33:06--  https://huggingface.co/leliuga/all-MiniLM-L6-v2-GGUF/resolve/main/all-MiniLM-L6-v2.F16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.160.5.109, 3.160.5.76, 3.160.5.102, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.160.5.109|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/68/86/688665eb5b2019706f226f4e2a0cb26c8f210c781549d129dc7f26d630ac2863/797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27all-MiniLM-L6-v2.F16.gguf%3B+filename%3D%22all-MiniLM-L6-v2.F16.gguf%22%3B&Expires=1753993986&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzk5Mzk4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzY4Lzg2LzY4ODY2NWViNWIyMDE5NzA2ZjIyNmY0ZTJhMGNiMjZjOGYyMTBjNzgxNTQ5ZDEyOWRjN2YyNmQ2MzBhYzI4NjMvNzk3YjcwYzRlZGY4NTkwN2ZlMGE0OWViODU4MTEyNTZmNjVmYTBmN2JmNTIxNjZiMTQ3ZmQxNmJlMmJlNDY2Mj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ner9E3yxQs26uUJgChEX%7E80dpi11rv54An2b-ccW04f80kZQw%7EP4Z2Qh2u62tDJBJ0S7aM4jr4qbEeQN3SbY-TW6yPw8%7EEFJNFAWPIPOrgJyKfnSQIGONAZJV0WpPBRIhq8Udt6bhVlr3rY3tQa2pVeWKhAN33eYc43zdwPLkWyMhcQbCIKcXVRid-f8jouxFzDL5NfpBeikKiphBdyK-5Unxj4EV5NqrMdAClQx1pU6m3XrpxmS1iqFonmSpxnJ9DjiLaiXO%7Ek32pQeg185%7Eb6TafqWN-GDwmzpIzXRKTJqEIG2ustUvO3bCWjPXZ6EEZMskR8xhCLoJx0azuzchw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-07-31 19:33:06--  https://cdn-lfs-us-1.hf.co/repos/68/86/688665eb5b2019706f226f4e2a0cb26c8f210c781549d129dc7f26d630ac2863/797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27all-MiniLM-L6-v2.F16.gguf%3B+filename%3D%22all-MiniLM-L6-v2.F16.gguf%22%3B&Expires=1753993986&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzk5Mzk4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzY4Lzg2LzY4ODY2NWViNWIyMDE5NzA2ZjIyNmY0ZTJhMGNiMjZjOGYyMTBjNzgxNTQ5ZDEyOWRjN2YyNmQ2MzBhYzI4NjMvNzk3YjcwYzRlZGY4NTkwN2ZlMGE0OWViODU4MTEyNTZmNjVmYTBmN2JmNTIxNjZiMTQ3ZmQxNmJlMmJlNDY2Mj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ner9E3yxQs26uUJgChEX%7E80dpi11rv54An2b-ccW04f80kZQw%7EP4Z2Qh2u62tDJBJ0S7aM4jr4qbEeQN3SbY-TW6yPw8%7EEFJNFAWPIPOrgJyKfnSQIGONAZJV0WpPBRIhq8Udt6bhVlr3rY3tQa2pVeWKhAN33eYc43zdwPLkWyMhcQbCIKcXVRid-f8jouxFzDL5NfpBeikKiphBdyK-5Unxj4EV5NqrMdAClQx1pU6m3XrpxmS1iqFonmSpxnJ9DjiLaiXO%7Ek32pQeg185%7Eb6TafqWN-GDwmzpIzXRKTJqEIG2ustUvO3bCWjPXZ6EEZMskR8xhCLoJx0azuzchw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 108.156.184.28, 108.156.184.97, 108.156.184.115, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|108.156.184.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45949216 (44M) [application/octet-stream]\n",
            "Saving to: â€˜all-MiniLM-L6-v2.F16.ggufâ€™\n",
            "\n",
            "all-MiniLM-L6-v2.F1 100%[===================>]  43.82M  88.8MB/s    in 0.5s    \n",
            "\n",
            "2025-07-31 19:33:07 (88.8 MB/s) - â€˜all-MiniLM-L6-v2.F16.ggufâ€™ saved [45949216/45949216]\n",
            "\n",
            "File â€˜README.mdâ€™ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"r50k_base\")\n",
        "\n",
        "embeddings = LlamaCppEmbeddings(model_path=\"/content/all-MiniLM-L6-v2.F16.gguf\")\n",
        "\n",
        "model = Llama(model_path=\"/content/Qwen3-4B-128K-Q8_0.gguf\",\n",
        "            n_gpu_layers=0,\n",
        "            temperature=0.1,\n",
        "            top_p = 0.5,\n",
        "            n_ctx=8192,\n",
        "            max_tokens=600,\n",
        "            repeat_penalty=1.7,\n",
        "            stop=[\"<|im_end|>\",\"Instruction:\",\"### Instruction:\",\"###<user>\",\"</user>\"],\n",
        "            verbose=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3gv_WJRXzwT",
        "outputId": "66401503-990c-48b2-ddd5-fb8c7770f24b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 23 key-value pairs and 101 tensors from /content/all-MiniLM-L6-v2.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = bert\n",
            "llama_model_loader: - kv   1:                               general.name str              = all-MiniLM-L6-v2\n",
            "llama_model_loader: - kv   2:                           bert.block_count u32              = 6\n",
            "llama_model_loader: - kv   3:                        bert.context_length u32              = 512\n",
            "llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384\n",
            "llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536\n",
            "llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000\n",
            "llama_model_loader: - kv   8:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv   9:                      bert.attention.causal bool             = false\n",
            "llama_model_loader: - kv  10:                          bert.pooling_type u32              = 1\n",
            "llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100\n",
            "llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103\n",
            "llama_model_loader: - type  f32:   63 tensors\n",
            "llama_model_loader: - type  f16:   38 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = F16\n",
            "print_info: file size   = 43.10 MiB (16.02 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 3\n",
            "load: control token:    101 '[CLS]' is not marked as EOG\n",
            "load: control token:    103 '[MASK]' is not marked as EOG\n",
            "load: control token:      0 '[PAD]' is not marked as EOG\n",
            "load: control token:    100 '[UNK]' is not marked as EOG\n",
            "load: control token:    102 '[SEP]' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 5\n",
            "load: token to piece cache size = 0.2032 MB\n",
            "print_info: arch             = bert\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 512\n",
            "print_info: n_embd           = 384\n",
            "print_info: n_layer          = 6\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 12\n",
            "print_info: n_rot            = 32\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 32\n",
            "print_info: n_embd_head_v    = 32\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 384\n",
            "print_info: n_embd_v_gqa     = 384\n",
            "print_info: f_norm_eps       = 1.0e-12\n",
            "print_info: f_norm_rms_eps   = 0.0e+00\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 1536\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 0\n",
            "print_info: pooling type     = 1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 512\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 22M\n",
            "print_info: model params     = 22.57 M\n",
            "print_info: general.name     = all-MiniLM-L6-v2\n",
            "print_info: vocab type       = WPM\n",
            "print_info: n_vocab          = 30522\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 101 '[CLS]'\n",
            "print_info: EOS token        = 102 '[SEP]'\n",
            "print_info: UNK token        = 100 '[UNK]'\n",
            "print_info: SEP token        = 102 '[SEP]'\n",
            "print_info: PAD token        = 0 '[PAD]'\n",
            "print_info: MASK token       = 103 '[MASK]'\n",
            "print_info: LF token         = 0 '[PAD]'\n",
            "print_info: EOG token        = 102 '[SEP]'\n",
            "print_info: max token length = 21\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (f16) (and 100 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =    43.10 MiB\n",
            "...............................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 0\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.mask_token_id': '103', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.seperator_token_id': '102', 'tokenizer.ggml.unknown_token_id': '100', 'tokenizer.ggml.model': 'bert', 'tokenizer.ggml.eos_token_id': '102', 'general.architecture': 'bert', 'bert.block_count': '6', 'bert.attention.layer_norm_epsilon': '0.000000', 'bert.context_length': '512', 'bert.feed_forward_length': '1536', 'bert.embedding_length': '384', 'tokenizer.ggml.cls_token_id': '101', 'tokenizer.ggml.token_type_count': '2', 'bert.attention.head_count': '12', 'tokenizer.ggml.bos_token_id': '101', 'general.file_type': '1', 'general.name': 'all-MiniLM-L6-v2', 'bert.attention.causal': 'false', 'bert.pooling_type': '1'}\n",
            "Using fallback chat format: llama-2\n",
            "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the README file\n",
        "\n",
        "loader = TextLoader(\"/content/README.md\")\n",
        "\n",
        "# Create documents and split into chunks\n",
        "documents = loader.load()\n",
        "text_splitter = TokenTextSplitter(chunk_size=150, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "print(len(texts))\n",
        "\n",
        "# Create vectorstore\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "NmvS1SEUc7hT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0112f1-5a27-4213-d5ae-c3e08c925ff4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import search\n",
        "# Default is Similarity search\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Using Maximal Marginal Relevance\n",
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
        "\n",
        "print(\"\", *retriever.invoke(\"What is the author email?\"), sep=f\"\\n\\n{'='*100}\\n\\nn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpmeKxN6e_Ti",
        "outputId": "356dc76f-729c-41b5-a5bb-c6eb6c875c0c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "npage_content=' Enjoyed this content?\n",
            "Explore more of my work on [Medium](https://medium.com/@erickcalderin) \n",
            "\n",
            "I regularly share insights, tutorials, and reflections on tech, AI, and more. Your feedback and thoughts are always welcome!\n",
            "' metadata={'source': '/content/README.md'}\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "npage_content=' ```bash\n",
            "    python -m mysql_ingestion.initialize_data\n",
            "    ```\n",
            "\n",
            "4. Go to:\n",
            "\n",
            "```bash\n",
            "http://localhost:8501/\n",
            "```\n",
            "\n",
            "### Lint\n",
            "Style the code with Ruff:\n",
            "\n",
            "```bash\n",
            "ruff format .\n",
            "ruff check . --fix\n",
            "```\n",
            "### Remove the containers\n",
            "\n",
            "```bash\n",
            "docker-compose down\n",
            "```\n",
            "\n",
            "## Contact\n",
            "**LinkedIn:** https://www.linkedin.com/in/erick-calderin-5bb6963b/  \n",
            "**e-mail:** edcm.erick@gmail.com\n",
            "\n",
            "##' metadata={'source': '/content/README.md'}\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "npage_content=' prompt engineering, chaining logic, and interaction with the LLM.\n",
            "\n",
            "* MySQL: Hosts the structured database, which includes employee records and related information.\n",
            "\n",
            "* Streamlit: Provides a responsive and intuitive front-end for real-time query input and response display.\n",
            "\n",
            "ðŸ§  Prompt Engineering Techniques\n",
            "The success of the SQL generation relies heavily on a carefully crafted prompting strategy:\n",
            "\n",
            "* Few-Shot Prompting:\n",
            "The prompt includes several hand-picked examples of natural language questions and their corresponding SQL translations.\n",
            "These examples demonstrate the expected structure, column naming conventions, and logic patterns needed for the LLM to generalize correctly.\n",
            "\n",
            "* Few-Shot Example Selector:\n",
            "To avoid overwhelming the model' metadata={'source': '/content/README.md'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain construction"
      ],
      "metadata": {
        "id": "xGrBKbpE4T_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any\n",
        "\n",
        "@dataclass(kw_only=True, frozen=True, slots=False)\n",
        "class QwenQnA:\n",
        "\n",
        "    vectorstore: Any\n",
        "    model: Any\n",
        "    search_kwargs: dict\n",
        "\n",
        "\n",
        "    def _get_messages(self, context: str, question: str)->list[dict]:\n",
        "        return [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Answer the question based on the context.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Question: {question}\\n\\nContext: {context}\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "\n",
        "    def _get_context(self, question: str)->str:\n",
        "        documents: list = self._get_retriever().invoke(question)\n",
        "        return \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "    def _get_retriever(self):\n",
        "        return self.vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs=self.search_kwargs\n",
        "        )\n",
        "\n",
        "    def invoke(self, question: str)->str:\n",
        "        context: str = self._get_context(question)\n",
        "\n",
        "        output = self.model.create_chat_completion(\n",
        "            messages=self._get_messages(context, question),\n",
        "            max_tokens=500,\n",
        "            stop=[\"</s>\",\"[/INST]\",\"/INST\",'<|eot_id|>','<|end|>'],\n",
        "            temperature = 0.1,\n",
        "            repeat_penalty = 1.4\n",
        "        )\n",
        "\n",
        "        return output[\"choices\"][0][\"message\"][\"content\"]"
      ],
      "metadata": {
        "id": "FgEqLzLg4WQj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "QwenQnA(\n",
        "    vectorstore=vectorstore,\n",
        "    model=model,\n",
        "    search_kwargs={\"k\": 3}\n",
        ").invoke(\"What are the ruff rules?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "P2vGQCO0Ry4_",
        "outputId": "a167b6a5-d10f-44f0-dbc2-05cef934d0ce"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6min 41s, sys: 586 ms, total: 6min 41s\n",
            "Wall time: 5min 58s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<think>\\nOkay, the user is asking about \"the ruff rules\" based on the provided context. Let me look through the content again.\\n\\nThe context mentions a section titled \"Lint\" where it says to style code with Ruff using two commands: `ruff format .` and `ruff check --fix`. Then there\\'s also mention of database schema injection and prompt templates with LangChain. But the question is specifically about Ruff rules. \\n\\nWait, in the context under the Lint section, they use Ruff for formatting and checking code. The commands given are to run Ruff format on the current directory and then check with fixes enabled. However, the user\\'s query refers to \"the ruff rules,\" which might be a misunderstanding or confusion between the command-line tools and some other concept.\\n\\nLooking back at the context, there isn\\'t any explicit mention of specific rules that Ruff enforces. The text talks about using Ruff for code styling and checking with fixes, but it doesn\\'t list out what those rules are. So maybe \"ruff rules\" here is a term used in another part not shown? Or perhaps they\\'re referring to the general guidelines or best practices when using Ruff?\\n\\nAlternatively, could there be an error in interpreting terms from the context? The user might have thought that \\'Ruff\\' refers to some set of coding standards. But according to the given information, it\\'s just a tool for formatting and checking code style.\\n\\nSo based on what is provided here, I should explain that Ruff rules aren\\'t detailed in this specific text but are used for styling with `ruff format` and static analysis via `ruff check`. The commands mentioned help maintain clean code by applying these rules. However, the actual list of rules would depend on the configuration set up when using Ruff.\\n</think>\\n\\nThe context provided does not explicitly detail \"the ruff rules\" but describes how to use **Ruff** (a Python linter and formatter) in a workflow for code styling:\\n\\n1. **Code Formatting**:  \\n   ```bash\\n   ruff format .\\n   ```\\n   This command formats the code according to Ruff\\'s default style guidelines, ensuring consistency across files.\\n\\n2. **Static Code Checking & Fixing**:  \\n   ```bash\\n   ruff check . --fix\\n   ```\\n   This runs static analysis on the code (checking for issues like syntax errors or PEP8 violations) and applies fixes automatically if enabled (`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}