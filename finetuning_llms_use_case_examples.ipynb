{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLMs: Use Case Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pub.towardsai.net/fine-tuning-llms-use-case-examples-2042d924c5b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install -q transformers peft accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Erick\\miniconda3\\envs\\finetuning_use_cases\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET TYPE: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "DATASET INFO: dict_items([('train', Dataset({\n",
      "    features: ['id', 'translation'],\n",
      "    num_rows: 210173\n",
      "}))])\n",
      "DATASET COL NAMES: {'train': ['id', 'translation']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "source_lang = \"en\"\n",
    "target_lan = \"fr\"\n",
    "dataset = load_dataset(\n",
    "    \"kde4\", \n",
    "    lang1=source_lang, \n",
    "    lang2=target_lan, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"DATASET TYPE: {type(dataset)}\")\n",
    "print(f\"DATASET INFO: {dataset.items()}\")\n",
    "print(f\"DATASET COL NAMES: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['10', '11', '12'],\n",
       " 'translation': [{'en': 'translate', 'fr': 'traduction'},\n",
       "  {'en': 'The Babel & konqueror; plugin',\n",
       "   'fr': 'Le module externe Babel pour & konqueror;'},\n",
       "  {'en': 'Using the Babelfish plugin',\n",
       "   'fr': 'Utilisation du module externe Babelfish'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take three values at random indexes.\n",
    "dataset[\"train\"][10:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets = dataset[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Erick\\miniconda3\\envs\\finetuning_use_cases\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length: int = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,                 # Define input text.\n",
    "        text_target=targets,    # Define \"labels\".\n",
    "        max_length=max_length, \n",
    "        truncation=True,        # Truncate texts to the same size.\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "# Map preprocess function to every dataset (train/validation)\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names, # Remove additional columns (try to run map function with and without remove_columns param to explore the results).\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erick\\AppData\\Local\\Temp\\ipykernel_27280\\3195440411.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70935' max='70935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70935/70935 2:43:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.339900</td>\n",
       "      <td>1.292833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.028800</td>\n",
       "      <td>1.048172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.927809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Erick\\miniconda3\\envs\\finetuning_use_cases\\Lib\\site-packages\\transformers\\modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70935, training_loss=1.1670829886419456, metrics={'train_runtime': 9808.588, 'train_samples_per_second': 57.854, 'train_steps_per_second': 7.232, 'total_flos': 6060553336848384.0, 'train_loss': 1.1670829886419456, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, AutoModelForSeq2SeqLM, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) \n",
    "\n",
    "# Data collators are objects that will form a batch by using a list of dataset\n",
    "# elements as input. These elements are of the same type as the elements of \n",
    "# train_dataset or eval_dataset.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
    "\n",
    "# Create an object for setting training arguments.\n",
    "model_args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_checkpoint}-finetuned-{source_lang}-to-{target_lan}\",  # Finetuned model name.\n",
    "    eval_strategy=\"epoch\",    # Defines when to run evaluation.\n",
    "    learning_rate=2e-4,             # Learning rate.\n",
    "    per_device_train_batch_size=8,  # Batch size per GPU for training if GPU is available, else per CPU core.\n",
    "    per_device_eval_batch_size=8,   # Same thing but for evaluation.\n",
    "    weight_decay=0.02,              # The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.\n",
    "    save_total_limit=3,             # Limits total amount of checkpoints. In this case, after training you will have only three checpoints.\n",
    "    num_train_epochs=3,             # Total number of training epochs.\n",
    "    predict_with_generate=True      # Whether to use generate to calculate generative metrics like ROUGE or BLEU.\n",
    ")\n",
    "\n",
    "# Create an object for training the LLM.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    model_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    #compute_metrics=compute_metrics  # This is a function that is used for computing metrics available at the github repo.\n",
    ")\n",
    "\n",
    "# This will start the training process. Be patient :)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Erick\\miniconda3\\envs\\finetuning_use_cases\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, mon nom est Erick & #160;!\n"
     ]
    }
   ],
   "source": [
    "# Now model_checkpoint (name) is the path to the finetuned model.\n",
    "# Your model is probably saved to the same folder from which you ran the training.\n",
    "my_model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr-finetuned-en-to-fr/checkpoint-70935\"\n",
    "my_model = AutoModelForSeq2SeqLM.from_pretrained(my_model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(my_model_checkpoint, return_tensors=\"pt\")\n",
    "\n",
    "text = \"Hello, my name is Erick!\"\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "result = my_model.generate(**tokenized_text)\n",
    "print(tokenizer.decode(result[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this issue, we need to clear the data before tokenizing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ceanining the input texts for the training.\n",
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    return re.sub('«&#160;', '', text) # You can define your expression/s here.\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [clean_text(ex[\"en\"]) for ex in examples[\"translation\"]]\n",
    "    targets = [clean_text(ex[\"fr\"]) for ex in examples[\"translation\"]]\n",
    "    return tokenizer(\n",
    "        inputs,                \n",
    "        text_target=targets,    \n",
    "        max_length=max_length, \n",
    "        truncation=True,       \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning_use_cases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
