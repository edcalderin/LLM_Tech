{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPf2R5A2vHrW4DX/IiTt31V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edcalderin/LLM_Tech/blob/master/unleash_mistral7b_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unleash Mistral 7Bâ€™ Power: How to Efficiently Fine-tune a LLM on Your Own Data"
      ],
      "metadata": {
        "id": "TxheLP3Q2qjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QezHbZKjUPU",
        "outputId": "ba82a411-d01b-4a18-ff60-b3df4297a57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device name: Tesla T4\n",
            "Device memory: 14.74 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device_properties = torch.cuda.get_device_properties()\n",
        "print(\"Device name:\", device_properties.name)\n",
        "print(\"Device memory:\", round(device_properties.total_memory/(1024**3), 2), \"GB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "U-15uQrTmpGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes datasets peft"
      ],
      "metadata": {
        "id": "kpcwkwwtmw_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"fp4\")\n",
        "\n",
        "model_id: str = \"mistralai/Mistral-7B-v0.3\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ],
      "metadata": {
        "id": "eOm5koDPmpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(query: str, model, tokenizer) -> str:\n",
        "  device = \"cuda:0\"\n",
        "\n",
        "  prompt_template = \"\"\"\n",
        "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "  ### Question:\n",
        "  {query}\n",
        "\n",
        "  ### Answer:\n",
        "  \"\"\"\n",
        "  prompt = prompt_template.format(query=query)\n",
        "\n",
        "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "\n",
        "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  return (decoded[0])"
      ],
      "metadata": {
        "id": "ZSe5AtJfq_Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_completion(query=\"Will capital gains affect my tax bracket?\", model=model, tokenizer=tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "5fusXJ4dq41P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"gbharti/finance-alpaca\", split='train[:10%]')\n",
        "\n",
        "# Explore the data\n",
        "df = data.to_pandas()\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "8AiHZhtXrN5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate a prompt text based on a data point\n",
        "def generate_prompt(data_point: dict):\n",
        "    \"\"\"\n",
        "    Generate input text based on a prompt, task instruction, (context info.), and answer.\n",
        "    Args:\n",
        "        data_point: Data point\n",
        "    Returns\n",
        "        Tokenized prompt\n",
        "    \"\"\"\n",
        "    # Check if the data point has additional context information\n",
        "    if data_point['input']:\n",
        "        # Create a text with instruction, input, and response\n",
        "        text = 'Below is an instruction that describes a task, paired with an input that provides' \\\n",
        "               ' further context. Write a response that appropriately completes the request.\\n\\n'\n",
        "        text += f'### Instruction:\\n{data_point[\"instruction\"]}\\n\\n'\n",
        "        text += f'### Input:\\n{data_point[\"input\"]}\\n\\n'\n",
        "        text += f'### Response:\\n{data_point[\"output\"]}'\n",
        "\n",
        "    # If there's no additional context\n",
        "    else:\n",
        "        # Create a text with just instruction and response\n",
        "        text = 'Below is an instruction that describes a task. Write a response that ' \\\n",
        "               'appropriately completes the request.\\n\\n'\n",
        "        text += f'### Instruction:\\n{data_point[\"instruction\"]}\\n\\n'\n",
        "        text += f'### Response:\\n{data_point[\"output\"]}'\n",
        "    return text\n",
        "\n",
        "# Add the \"prompt\" column in the dataset by applying the generate_prompt function to each data point\n",
        "text_column = [generate_prompt(data_point) for data_point in data]\n",
        "data = data.add_column(\"prompt\", text_column)\n",
        "\n",
        "# Shuffle the dataset with a specified seed\n",
        "data = data.shuffle(seed=1234)\n",
        "\n",
        "# Tokenize the \"prompt\" column using the tokenizer, processing the data in batches\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "\n",
        "# Split the dataset into training and testing subsets, with 90% for training and 10% for testing\n",
        "data = data.train_test_split(test_size=0.1)\n",
        "train_data = data[\"train\"]\n",
        "test_data = data[\"test\"]"
      ],
      "metadata": {
        "id": "ycesqOdnrPAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary function from the \"peft\" library to prepare a model for k-bit training\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# Enable gradient checkpointing for the model\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Prepare the model for k-bit training using the \"prepare_model_for_kbit_training\" function\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Define a function to print the number of trainable parameters in the model\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    # Iterate through model parameters\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    # Print the number of trainable parameters, total parameters, and the percentage of trainable parameters\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "# Import necessary components from the \"peft\" library\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Define a configuration for the LoRA (Learnable Requantization Activation) method\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                                   # Number of quantization levels\n",
        "    lora_alpha=32,                         # Hyperparameter for LoRA\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Modules to apply LoRA to\n",
        "    lora_dropout=0.05,                     # Dropout probability\n",
        "    bias=\"none\",                           # Type of bias\n",
        "    task_type=\"CAUSAL_LM\"                  # Task type (in this case, Causal Language Modeling)\n",
        ")\n",
        "\n",
        "# Get a model with LoRA applied to it using the defined configuration\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print the number of trainable parameters in the model after applying LoRA\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "d-tygvaVsUxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Set the pad_token of the tokenizer to be the same as the eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Create a trainer for fine-tuning a model\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,  # The model to be trained\n",
        "    train_dataset=train_data,  # Training dataset\n",
        "    eval_dataset=test_data,    # Evaluation dataset\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,        # Batch size per device during training\n",
        "        gradient_accumulation_steps=4,        # Number of gradient accumulation steps\n",
        "        warmup_steps=1,                       # Number of warm-up steps for learning rate\n",
        "        max_steps=100,                        # Maximum number of training steps\n",
        "        learning_rate=2e-4,                   # Learning rate\n",
        "        fp16=True,                            # Enable mixed-precision training\n",
        "        logging_steps=1,                      # Logging frequency during training\n",
        "\n",
        "        optim=\"paged_adamw_8bit\",             # Optimizer type\n",
        "        save_strategy=\"epoch\",                # Strategy for saving checkpoints\n",
        "        push_to_hub=True                      # Push to the Hugging Face model hub\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    # Data collator for language modeling task\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "6RF84XXbxW70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_completion(query=\"Will capital gains affect my tax bracket?\", model=model, tokenizer=tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "irROfgyvxb-6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}